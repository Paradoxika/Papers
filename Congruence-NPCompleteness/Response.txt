Dear editors and reviewers,

We thank you for your positive evaluation of our paper and your comments.

Below is a list of changes made to address the reviewers' comments.


============================

Reviewer #1: 


> Abstract: we provide here an NP-hardness proof --> by reading the abstract
>          one could understand that the paper does not prove the problem
>          to be in NP. I would replace hardness by completeness.

Done.


> Page 2, line 45: polynomial time algorithm --> polynomial-time algorithm

Fixed.

> Page 3, line 2: this short paper --> this paper

Done.


> Page 5: when explaining the congruence graph, I would add a small
>        example with a figure including the resulting congruence graph

This was indeed lacking.  Fixed.

> Page 5, line 46: as far as I know, explanation production can only be
>        done in time O(k \alpha(k,k)) for a proof with k literals.

The sentence "the explanation production is linear with respect to the
explanation size" was indeed a bit bold.  We are now more precise, explicitly
referring to the work of Nieuwenhuis and Oliveras for details.

Our implementation in veriT might be linear, but to state it, a proof would be needed, with a precise description of the algorithm, and this is outside the scope of the paper.

TODO: Maybe "Our implementation in VeriT might be linear (under assumptions that are slightly different from the work of Nieuwenhhuis and Oliveras), but..." . Otherwise, as it is, it gives the impression that we don't believe that their complexity result is correct. Also, I am assuming that the reviewer actually meant Omega(k alpha(k, k))... What is the actual result in the paper? If the paper's result is indeed O(k alpha(k,k)), then I suggest changing our sentence to "Our implementation in VeriT might be linear (and hence also in O(k \alpha(k,k)), but..."


> Page 7, Definition 7: the star in Assignment might confuse the reader
>        since it is very similar to the asterisk in the congruence
>        definition. I would use another symbol.

Indeed the overload is possibly confusing and unnecessary.  We changed Assignment* to AssignmentEqs.

> Page 9, line 19: cardinality constraint on the explanation -->
>                 cardinality constraint on the explanation size

Done.


> Page 9, Lemma 3: it would improve the readability to include an
>        extension of Figure 4 (or a figure similar to it) where the
>        tautological clauses are depicted.

Good suggestion.
We added such an example and two figures.
We also rewrote the proof to improve clarity.

TODO: proof read the proof
++++++++++++++++++++++++++++++++
Bruno's comment: I think the proof indeed became clearer. 
Thanks for that! Please double check any new equations. 
I didn't have time to do it as thoroughly as I would have liked. 
Regarding style, I would prefer not to use "we have", "we can see that", and so on in a mathematical proof. 

I see no problem in using "we" in a paper to report subjective things like "we conjecture that...", 
"we have chosen to do that in this way because...", but a mathematical proof is objective. 
Subjective relativizations of objective truths are unnecessary and often uselessly verbose.

The paragraph starting with "Thus far, we have..." is confusing. It is not clear whether you are adding n+m-1 equations to E or if those n+m-1 equations are in E_I. 

A proof is like a program. In the same way that we should avoid an imperative mutable coding style with state and side effects and prefer an immutable stateless functional declarative coding style, we should avoid changing named entities (e.g. E in this case), or even giving the impression that we are changing them, in our proofs. Sentences containing temporal expressions (e.g. "Thus far", "we add") give this impression.

"Additionaly, we add" sounds pleonastic.

Anyway, these are just minor improvement suggestions, depending a lot on personal taste. I'm satisfied with the
current proof and leave it up to you to decide whether you would like to do any further changes.
++++++++++++++++++++++++++++++++++++++++++++++++

> Page 10, line 36: Roberto Nieuwenhuis --> Robert Nieuwenhuis

Fixed.

> Page 11, line 5: Jovanovi --> Jovanovic

Fixed.


==========================

Reviewer #2: 


> Generally lemma 3 is the most important in the article, 
> but in contrast to other parts of the article, it goes too fast. 
> There is a crucial point there about enforcing each variable x to 
> be exactly one of T or F, via an additional cardinality constraint 
> and tautological clauses (x \/ !x). But it is not clear to me how 
> this cardinality constraint looks like in EUF. It seems that what 
> they want to achieve is 
>
> (x = T) + (x = F) = 1, 
>
> and they do it by adding 
>
> (x = T) + (x = F) <= 1 
>
> together with 
>
> (x = T) \/ (X = F).
>
> If yes, then how does the cardinality constraint fall into 
> the conjunctive fragment of EUF? 

The cardinality constraint is not part of the encoded problem.
It is part of the proof as one side of the if and only if statement.
When the cardinality constraint |E'| <= 3n + 4m - 1 is assumed (together with the structure of E' and the congruence closure problem), 
it follows that either (x = T) or (x = F) is in an explanation.

We rewrote the proof to clarify this issue and improve clarity in general.

> Also, why isn't the inequality sufficient? (i.e., why enforce a full assignment ? ).

We need to have equality in order to rule out sets of equations as explanations that have x=T and x=F in them,
and thereby do not correspond to assignments.
We added an example right after the proof of Lemma 3 making this argument explicit.
  
> abstract: "'to our best knowledge' => 'to the best of our knowledge' 
> (see https://answers.yahoo.com/question/index?qid=20130720053631AAXDsHg)

Fixed.

> p 2: 'searching for short paths' => did you mean shortest paths ? 

Yes and no.
We used an adaptation of a shortest path algorithm, but since the weights of the graph dynamically changed, 
we were not guaranteed to construct shortest paths in the final graph.
It is beyond the scope of this paper to describe this method in detail.
However, we re-wrote the paragraph to make the method more explicit.

TODO: @Bruno look at Andreas' changes to this
Bruno's comment: you seem to conclude that the NP-completeness is a negative answer to whether the dynamically weighted shortest path algorithm can find the shortest path. 
But this conclusion is only possible if you additionally argue/mention that the dynamically weighted shortest path algorithm is polynomial-time. Is it? If so, this has to be mentioned in your paragraph.


> p 4: Def 2: is 'compatible' a standard term ? I always saw 'congruent'. 

Yes, it is a standard term. See, for instance:
https://en.wikipedia.org/wiki/Quotient_algebra#Compatible_relation
https://en.wikipedia.org/wiki/Congruence_relation
(and the algebra books referrenced there.)

We suspect that "compatible" is the original term from Mathematics, where a congruence relation is defined as: an equivalence relation that is also compatible. 

Also, replacing "compatible" by 'congruent' would lead to an apparently circular definition, since we would have to say: a congruence relation is an equivalence relation that is also 'congruent'.

For these two reasons, we consider it more appropriate to use the term "compatible" than the term "congruent" to refer to that axiom, even if "congruent" might be more common in some communities.


> "(identity of two terms can be checked in
> constant time), and identity of two function symbols can be checked in constant
> time." since functions are terms then isn't this repetitive ? 

As entailed by our definition 1, 
we use the usual terminology of first-order logic. 
A term is either a constant/variable symbol or an 
n-ary function symbol applied to n terms. 
Therefore, a function symbol is generally not a term, except in the 
case of nullary function symbols (i.e. constants). 

Nevertheless, we agree that the sentence was not optimal. We improved it to:

"terms are DAGs with maximal
sharing (i.e. identity of atomic symbols and complex terms can be checked in constant time)"


> p 6: the sentence 'One could hope...' indeed raises the hopes that with 
> such redundance it can be achieved. Perhaps you can show an example that 
> demonstrates why this intuition is wrong ? 

This would require us to define more precisely the ideas that might lead one to hope that, in order to show some examples why those naive ideas do not always result in the shortest explanation. But the benefit of doing that would not be worth the distraction that it would cause.

Nevertheless, we have slightly reformulated that paragraph to better convey the message that the hope is naive. The new paragraph is reproduced below:

"One could (naively) hope to conceive a different
congruence closure algorithm generating the smallest explanation in polynomial
time. For example, one might attempt to modify the iterative removal algorithm; or attempt to modify shortest path algorithms and apply them to congruence graphs enriched with redundant equations as labels. However, such attempts would be futile. As proven in the next section, the corresponding decision problem is NP-hard. "



> p 8: The bottom dotted edge on the left is wrong? (t1(x3) --- t1(\bot_3))

Well spotted.
Fixed.

> Please use \qed at the end of proofs. 

Fixed

==========================

Reviewer #3: 


> p3, 2: "short paper" -> "short article"

Fixed to "article". Reviewer 1 requested the removal of "short"


> p3, 30: "null arity" -> "arity zero", or "zero-ary"/"nullary"

Fixed to "nullary".


> p4, 13: the empty set is not a congruence relation, the authors
> probably mean the diagonal set { (x, x) | x in T }

Fixed

> p5, 20: for functions of arity > 1, congruence edges have to be
> labeled with a vector of equalities

The label of a congruence edge is the equality itself, 
e.g. if there is a congruence edge between terms f(a,b) and f(c,d), then the label is f(a,b) = f(c,d).
It is also an option to use explanations as labels, e.g. <a=c,b=d>.
However, there can be multiple explanations for a congruence edge.
Ideally, we would like to label it with the shortest one, but we do not know which is the shortest one before solving the problem.
If we would know, a shortest path algorithm could find shortest explanations, if the weights correspond to sizes of explanations.

> p5, 21: "logic" -> "logical"

Fixed.
