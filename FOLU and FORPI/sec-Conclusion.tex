\section{Conclusions and Future Work}\label{sec:conclusion}

The main contribution of this paper is the lifting of the propositional proof compression algorithms {\LowerUnits} and {\RPI} to the first-order case. 
As indicated in Sections \ref{sec:LUChallenges} and \ref{sec:FORPIChallenges}, these generalizations are challenging, because unification instantiates literals. 
Consequently, a node may be regularizable even if its resolved literals are not syntactically equal to any safe literal. 
Therefore, unification must be taken into account when collecting safe literals and marking nodes for deletion.
Similarly, not all unit clauses can be lowered after instantiation.

We first evaluated the algorithms on all 308 real proofs that the \texttt{SPASS} theorem prover (with only standard resolution enabled) was capable of generating when executed on unsatisfiable TPTP problems without equality. 
The compression achieved by the first-order variants of {\LowerUnits} and {\FORPI} was not as good as the compression achieved by their propositional variants, due to the fact that the 308 proofs were too short (less than 32 resolutions) to contain a significant amount of irregularities. 
In contrast, the propositional proofs used in the evaluation of the propositional {\RPI} algorithm had thousands (and sometimes hundreds of thousands) of resolutions. 

Our second evaluation used larger, but randomly generated, proofs. 
The compression achieved by {\GFOLU} and {\FORPI} in a short amount of time on this data set was compatible with our expectations and previous experience in the propositional level. 
The obtained results indicate that these algorithms are a promising compression technique to be reconsidered when first-order theorem provers become capable of producing larger proofs. 
Although we carefully selected generation probabilities in accordance with frequencies observed in real proofs, it is important to note that randomly generated proofs may still differ from real proofs in shape and may be more or less likely to contain irregularities exploitable by our algorithm. Resolution restrictions and refinements (e.g. ordered resolution %\cite{Maslov1964,KowalskiHayes1969,OrderedRes}, 
\cite{hsiang1991proving,OrderedRes}, hyper-resolution \cite{HyperResolution,robinson1965automatic}, unit-resulting resolution \cite{UnitResultingResolution,prover9-mace4}) may result in longer chains of resolutions and, therefore, in proofs with a possibly larger height to length ratio. 
As the number of irregularities increases with height, such proofs could have a higher number of irregularities in relation to length.
It would also be interesting to study these algorithms to determine how much they reduce the required set of axioms on average (as in Example \ref{ex:unif}), and to determine if a node often has a large set of safe literals in a proof.

In this paper, for the sake of simplicity, we considered a pure resolution calculus without restrictions, refinements or extensions. 
However, in practice, theorem provers do use restrictions and extensions. 
It is conceptually easy to adapt the algorithm described here to many variations of resolution. 
For instance, restricted forms of resolution (e.g. ordered resolution, hyper-resolution, unit-resulting resolution) can be simply regarded as (chains of) unrestricted resolutions for the purpose of proof compression. 
The compression process would break the chains and change the structure of the proof, but the compressed proof would still be a correct unrestricted resolution proof, albeit not necessarily satisfying the restrictions that the input proof satisfied. 
In the case of extensions for equality reasoning using paramodulation-like inferences, it might be necessary to apply the paramodulations to the corresponding safe literals. 
Alternatively, equality inferences could be replaced by resolutions with instances of equality axioms, and the proof compression algorithm could be applied to the proof resulting from this replacement. 
Another common extension of resolution is the splitting technique \cite{WeidenbachSplitting}. 
When splitting is used, each split sub-problem is solved by a separate refutation, and the compression algorithm described here could be applied to each refutation independently.
