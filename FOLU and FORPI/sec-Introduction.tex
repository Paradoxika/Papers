\section{Introduction} 

%Recently, there has been interest in the combination of theorem provers and machine learning techniques (e.g. \cite{irving2016deepmath,kaliszyk2018reinforcement,DBLP:conf/lpar/LoosISK17}). 
Explainable artificial intelligence is a major challenge for the artificial intelligence community \cite{bonacina2017automated}.
As artificial intelligence systems are used in a wider range of applications with greater consequences, the need to justify and verify the choices made by these systems will grow as well.
In the logical approach to artificial intelligence, theorem provers provide explanations through verifiable proofs of the decisions that they make.
On the other hand, machine learning-based approaches often fail to explain why they produced a particular answer (see e.g., \cite{miller2019explanation}). 
In order to improve the ability to explain machine learning-based systems, there have been suggestions and attempts to combine machine learning with automated reasoning tools to generate explainable results \cite{bonacina2017automated,siebert2019corg}. 
The logical approach to artificial intelligence is no longer separate from the machine learning approach.
Good proofs are therefore useful for the successful combination of these approaches, and this paper aims to improve generated proofs through proof compression.

Proof production is a key feature for modern theorem provers. 
Proofs are explanations for unsatisfiability, and are crucial for applications that require certification of a prover's answers or that extract additional information from proofs (e.g. unsat cores, interpolants, instances of quantified variables).
Mature first-order automated theorem provers, commonly based on refinements and extensions of resolution and superposition calculi \cite{Vampire,EProver,Spass,spassT,Beagle,cruanes2015extending,prover9-mace4}, support proof generation. 
However, proof production is non-trivial \cite{SchulzAPPA}, and the most efficient provers do not necessarily generate the shortest proofs.
One reason for this is that efficient resolution provers use refinements that restrict the application of inference rules.
Although fewer clauses are generated and the search space is reduced, refinements may exclude short proofs whose inferences do not satisfy the restriction.

Proof compression techniques ameliorate the difficulties that automated reasoning tools encounter during proof generation. Such techniques can be integrated into theorem provers or external tools with minimal overhead. Moreover, proof compression techniques (like those described in this paper) may result in a stronger proof which uses a strict subset of the original axioms required, which could also be considered simpler. The problem of proof compression is also closely related to Hilbert's 24th Problem \cite{Hilbert24Problem}, which asks for criteria to judge the simplicity of proofs; proof length is one possible criterion. 


%Proof certification is an important challenge for the artificial intelligence community, and longer proofs are worse explanations than shorter proofs. 
There are also technical reasons to seek smaller proofs.
Longer proofs take longer to check, consume more memory during proof-checking, occupy more storage space and are harder to exchange, may have a larger unsat core (if more input clauses are used in the proof), and have a larger Herbrand sequent if more variables are instantiated \cite{B10,B16,ResolutionHerbrand,Reis}. Recent applications of SAT solvers to mathematical problems have resulted in very large proofs; e.g., the proof of a long-standing problem in combinatorics was initially 200GB \cite{heule2016solving}. Such proofs are hard to store, let alone validate. More practically, a restriction of 100GB of disk space per benchmark per solver prevented validation of proofs in the SAT 2014 competition \cite{clausal}. 
%Although the first example is extreme, the second is not. %, especially when users may wish to instantiate parallel solvers. 
The inability to write their results to disk renders these solvers useless in some cases. Moreover, even if the only direct improvement of shorter proofs is in the communication between systems, there are indirect benefits to the end-user of a tool e.g., in terms of its responsiveness. 


For propositional resolution proofs, as those typically generated by SAT- and SMT-solvers, there is a wide variety of proof compression techniques. Algebraic properties of the resolution operation that are potentially useful for compression were investigated in \cite{bwp10}.
Compression algorithms based on rearranging and sharing chains of resolution inferences have been
developed in \cite{Amjad07} and \cite{Sinz}.  Cotton \cite{CottonSplit} proposed an algorithm that
compresses a refutation by repeatedly splitting it into a proof of a heuristically chosen literal $\ell$
and a proof of $\dual{\ell}$, and then resolving them to form a new refutation.  The {\ReduceReconstruct} algorithm \cite{RedRec} searches for locally redundant
subproofs that can be rewritten into subproofs of stronger clauses and with fewer resolution steps.
Bar-Ilan et al. \cite{RP08} and Fontaine et al. \cite{LURPI} described a linear time proof compression algorithm based on partial
regularization, which removes an inference $\eta$ when it is redundant in the sense that its pivot literal already occurs as the pivot of another inference in every path from $\eta$ to the root of the proof.

In contrast, although proof output has been a concern in first-order automated reasoning for a longer time than in propositional SAT-solving, there has been much less work on simplifying first-order proofs. For tree-like sequent calculus proofs, algorithms based on cut-introduction \cite{BrunoLPAR,Hetzl} have been proposed. However, converting a DAG-like resolution or superposition proof, as usually generated by current provers, into a tree-like sequent calculus proof may increase the size of the proof. For arbitrary proofs in the Thousands of Problems for Theorem Provers (TPTP) \cite{TPTP} format (including DAG-like first-order resolution proofs), there is an algorithm \cite{LPARCzech} that looks for terms that occur often in any Thousands of Solutions from Theorem Provers (TSTP) \cite{TPTP} proof and abbreviates them. 

The work reported in this paper lifts successful propositional proof compression algorithms to first-order logic.
We first lift the {\LowerUnits} ({\LU}) algorithm \cite{LURPI}, which delays resolution steps with unit clauses, resulting in a new algorithm that we called {\SFOLowerUnits} ({\GFOLU}). 
Following this, we lift the \texttt{Recycle\-PivotsWithIntersection} ({\RPI}) algorithm \cite{LURPI}. 
\RPI improves the \texttt{RecyclePivots} ({\RP}) algorithm \cite{RP08} by detecting nodes that can be regularized even when they have multiple children.
Earlier versions of this work appeared in \cite{GFOLU,forpigcai}.

This paper is organized as follows. 
Section \ref{sec:res} introduces the well-known first-order resolution calculus with notations that are suitable for describing and manipulating proofs as first-class objects.
Section \ref{sec:PropositionalLU} describes the propositional \LowerUnits algorithm.
Section \ref{sec:LUChallenges} demonstrates some challenges of lowering units in the context of first-order logic.
Section \ref{sec:FOLU} describes a quadratic time approach to lifting units in first-order logic while Section \ref{sec:SimpleFOLU} demonstrates a simpler, linear time approach, \GFOLU.
We then repeat this structure for {\RPI}: Section \ref{Section:RPI} summarizes the propositional {\RPI} algorithm and Section \ref{sec:FORPIChallenges} discusses the challenges that arise in the first-order case (mainly due to unification), which are not present in the propositional case, and conclude with conditions useful for first-order regularization. 
Section \ref{sec:FORPI} describes an algorithm that overcomes these challenges. 
Section \ref{sec:exp} presents experimental results obtained by applying the first-order variant of \RPI and its combinations with {\GFOLU}, on hundreds of proofs generated with the {\SPASS} theorem prover on TPTP benchmarks \cite{TPTP} and on randomly generated proofs. 
Section \ref{sec:conclusion} concludes the paper.

It is important to emphasize that this paper targets proofs in a pure first-order resolution calculus (with resolution and factoring rules only), without refinements or extensions, and without equality rules. As most state-of-the-art resolution-based provers use variations and extensions of this pure calculus and there exists no common proof format, the presented algorithm cannot be directly applied to the proofs generated by most provers, and even {\SPASS} had to be specially configured to disable {\SPASS}'s extensions in order to generate pure resolution proofs for our experiments. By targeting the pure first-order resolution calculus, we address the common theoretical basis for the calculi of various provers. In the Conclusion (Section \ref{sec:conclusion}), we briefly discuss what could be done to tackle common variations and extensions, such as splitting and equality reasoning. Nevertheless, they remain topics for future research beyond the scope of this paper.
