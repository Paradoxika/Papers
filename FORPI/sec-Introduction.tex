\section{Introduction} 

First-order automated theorem provers, commonly based on refinements and extensions of resolution and superposition calculi \cite{Vampire,EProver,Spass,spassT,Beagle,cruanes2015extending,prover9-mace4}, have recently achieved a high degree of maturity. Proof production is a key feature that has been gaining importance, as proofs are crucial for applications that require certification of a prover's answers or that extract additional information from proofs (e.g. unsat cores, interpolants, instances of quantified variables). Nevertheless, proof production is non-trivial \cite{SchulzAPPA}, and the most efficient provers do not necessarily generate the shortest proofs. One reason for this is that efficient resolution provers use refinements that restrict the application of inference rules. Although fewer clauses are generated and the search space is reduced, refinements may exclude short proofs whose inferences do not satisfy the restriction.

Longer and larger proofs take longer to check, may consume more memory during proof-checking and occupy more storage space, and may have a larger unsat core, if more input clauses are used in the proof, and a larger Herbrand sequent, if more variables are instantiated \cite{TODO: HerbarandSequentExtraction Master Thesis, Book and MKM paper, also Expansion Tree Proofs by Martin and Giselle}. For these technical reasons, it is worth pursuing efficient algorithms that compress proofs after they have been found. Furthermore, the problem of proof compression is closely related to Hilbert's 24th Problem \cite{TODO:HilbertProblem24}, which asks for criteria to judge the simplicity of proofs. Proof length is arguably one possible criterion for some applications.

For propositional resolution proofs, as those typically generated by SAT- and SMT-solvers, there is a wide variety of proof compression techniques. Algebraic properties of the resolution operation that are potentially useful for compression were investigated in \cite{bwp10}.
Compression algorithms based on rearranging and sharing chains of resolution inferences have been
developed in \cite{Amjad07} and \cite{Sinz}.  Cotton \cite{CottonSplit} proposed an algorithm that
compresses a refutation by repeatedly splitting it into a proof of a heuristically chosen literal $\ell$
and a proof of $\dual{\ell}$, and then resolving them to form a new refutation.  The {\ReduceReconstruct} algorithm \cite{RedRec} searches for locally redundant
subproofs that can be rewritten into subproofs of stronger clauses and with fewer resolution steps.
\cite{RP08} and \cite{LURPI} described a linear time proof compression algorithm based on partial
regularization, which removes an inference $\eta$ when it is redundant in the sense that its pivot literal already occurs as the pivot of another inference in every path from $\eta$ to the root of the proof.

In contrast, although proof output has been a concern in first-order automated reasoning for a longer time than in propositional sat-solving, there has been much less work on simplifying first-order proofs. For tree-like sequent calculus proofs, algorithms based on cut-introduction \cite{BrunoLPAR,Hetzl} have been proposed. However, converting a DAG-like resolution or superposition proof, as usually generated by current provers, into a tree-like sequent calculus proof may increase the size of the proof. For arbitrary proofs in the Thousands of Problems for Theorem Provers (TPTP) \cite{TPTP} format (including DAG-like first-order resolution proofs), there is an algorithm \cite{LPARCzech} that looks for terms that occur often in any Thousands of Solutions from Theorem Provers (TSTP) \cite{TPTP} proof and abbreviates them. 


The work reported in this paper is part of a new trend that aims at lifting successful propositional proof compression algorithms to first-order logic. Our first target was the propositional {\LowerUnits} ({\LU}) algorithm \cite{LURPI}, which delays resolution steps with unit clauses, and we lifted it to a new algorithm that we called
{\SFOLowerUnits} 
({\GFOLU}) algorithm \cite{GFOLU}. Here we continue this line of research by lifting the 
%\RecyclePivotsIntersection 
\texttt{Recycle\-PivotsWithIntersection}
({\RPI}) algorithm \cite{LURPI}, which improves the \texttt{RecyclePivots} ({\RP}) algorithm \cite{RP08} by detecting nodes that can be regularized even when they have multiple children. 

%TODO: edit this again
Section \ref{sec:res} introduces the well-known first-order resolution calculus with notations that are suitable for describing and manipulating proofs as first-class objects. Section \ref{sec:Challenges} discusses the challenges that arise in the first-order case (mainly due to unification), which are not present in the propositional case, and conclude with conditions useful for first-order regularization. Section \ref{sec:FORPI} describes an algorithm that overcomes these challenges. Section \ref{sec:exp} presents experimental results obtained by applying this algorithm, and its combinations with {\GFOLU}, on hundreds of proofs generated with the {\SPASS} theorem prover on TPTP benchmarks \cite{TPTP} and on randomly generated proofs. Section \ref{sec:conclusion} concludes the paper.

It is important to emphasize that this paper targets proofs in a pure first-order resolution calculus (with resolution and factoring rules only), without refinements or extensions, and without equality rules. As most state-of-the-art resolution-based provers use variations and extensions of this pure calculus and there exists no common proof format, the presented algorithm cannot be directly applied to the proofs generated by most provers, and even {\SPASS} had to be specially configured to disable {\SPASS}'s extensions in order to generate pure resolution proofs for our experiments. By targeting the pure first-order resolution calculus, we address the common theoretical basis for the calculi of various provers. In the Conclusion (Section \ref{sec:conclusion}), we briefly discuss what could be done to tackle common variations and extensions, such as splitting and equality reasoning. Nevertheless, they remain topics for future research beyond the scope of this paper.
