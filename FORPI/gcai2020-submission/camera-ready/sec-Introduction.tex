\section{Introduction} 


Explainable artificial intelligence is a major challenge for the artificial intelligence community \cite{bonacina2017automated}.~As artificial intelligence systems are used in a wider range of applications with greater consequences, the need to justify and verify the choices made by these systems will grow as well.
In the logical approach to artificial intelligence, theorem provers provide explanations through verifiable proofs of the decisions that they make.
On the other hand, machine learning-based approaches often fail to explain why they produced a particular answer (see e.g., \cite{miller2019explanation}). 
In order to improve the ability to explain machine learning-based systems, there have been suggestions and attempts to combine machine learning with automated reasoning tools to generate explainable results \cite{bonacina2017automated,siebert2019corg}. 
The logical approach to artificial intelligence is no longer separate from the machine learning approach.
Good proofs are therefore useful for the successful combination of these approaches, and this paper aims to improve generated proofs through proof compression.

Proof production is a key feature for modern theorem provers. 
Proofs are explanations for unsatisfiability, and are crucial for applications that require certification of a prover's answers or that extract additional information from proofs (e.g. unsat cores, interpolants, instances of quantified variables).
Mature first-order automated theorem provers, commonly based on refinements and extensions of resolution and superposition calculi \cite{Vampire,EProver,Spass,spassT,prover9-mace4}, support proof generation. However, proof production is non-trivial \cite{SchulzAPPA}, and the most efficient provers do not necessarily generate the shortest proofs. 

Proof compression techniques ameliorate the difficulties that automated reasoning tools encounter during proof generation. Such techniques can be integrated into theorem provers or external tools with minimal overhead. Moreover, proof compression techniques (like those described in this paper) may result in a stronger proof which uses a strict subset of the original axioms required, which could also be considered simpler. The problem of proof compression is also closely related to Hilbert's 24th Problem \cite{Hilbert24Problem}, which asks for criteria to judge the simplicity of proofs; proof length is one possible criterion. 


There are also technical reasons to seek smaller proofs.
Longer proofs take longer to check, consume more memory during proof-checking, occupy more storage space and are harder to exchange, may have a larger unsat core (if more input clauses are used in the proof), and have a larger Herbrand sequent if more variables are instantiated \cite{B10,B16,ResolutionHerbrand,Reis}. Recent applications of SAT solvers to mathematical problems have resulted in very large proofs; e.g., the proof of a long-standing problem in combinatorics was initially 200GB \cite{heule2016solving}. Such proofs are hard to store, let alone validate. More practically, a restriction of 100GB of disk space per benchmark per solver prevented validation of proofs in the SAT 2014 competition \cite{clausal}. 
The inability to write their results to disk renders these solvers useless in some cases. Moreover, even if the only direct improvement of shorter proofs is in the communication between systems, there are indirect benefits to the end-user of a tool e.g., in terms of its responsiveness. 



For propositional resolution proofs, as those typically generated by SAT- and SMT-solvers, there is a wide variety of proof compression techniques. 
These techniques include investigating algebraic properties of resolution \cite{bwp10}, rearranging and sharing chains of resolution inferences \cite{Amjad07, Sinz}, and splitting a proof according a literal which may result in a compressed proof when recombined \cite{CottonSplit}.
Bar-Ilan et al. \cite{RP08} and Fontaine et al. \cite{LURPI} described a linear time proof compression algorithm based on partial
regularization, which removes an inference $\eta$ when it is redundant in the sense that its pivot literal already occurs as the pivot of another inference in every path from $\eta$ to the root of the proof.



By contrast, there has been much less work on simplifying first-order proofs. 
For arbitrary proofs in the Thousands of Problems for Theorem Provers (TPTP) \cite{TPTP} format (including DAG-like first-order resolution proofs), there is an algorithm \cite{LPARCzech} that looks for terms that occur often in any Thousands of Solutions from Theorem Provers (TSTP) \cite{TPTP} proof and abbreviates them. 


The work reported in this paper is part of a new trend that aims at lifting successful propositional proof compression algorithms to first-order logic. 
We first lifted the {\LowerUnits} ({\LU}) algorithm \cite{LURPI}, which delays resolution steps with unit clauses, resulting in a new algorithm that we called
{\SFOLowerUnits} 
({\GFOLU}) \cite{GFOLU}. Here we continue this line of research by lifting the 
%\RecyclePivotsIntersection 
\texttt{Recycle\-PivotsWithIntersection}
({\RPI}) algorithm \cite{LURPI}, which improves the \texttt{RecyclePivots} ({\RP}) algorithm \cite{RP08} by detecting nodes that can be regularized even when they have multiple children. 


Section \ref{sec:res} defines the first-order resolution calculus. Section \ref{sec:rpi} summarizes the propositional {\RPI} algorithm. 
Section \ref{sec:Challenges} discusses the challenges and conditions for partial regularization in the first-order case.
Section \ref{sec:exp} presents experimental results obtained by applying this algorithm, and its combinations with {\GFOLU}, on proofs generated by {\SPASS} \cite{Spass} and randomly generated proofs. Section \ref{sec:conclusion} concludes the paper.


