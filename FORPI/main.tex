\documentclass{llncs}
\usepackage{etex}

\usepackage{xcolor}
\usepackage{enumitem,amsmath,amssymb}
%\usepackage{breakurl}    % used for \url and \burl
\usepackage{url}
\usepackage[linesnumbered,boxed,noline,noend]{algorithm2e}
\def\defaultHypSeparation{\hskip.1in}

\usepackage{tikz}
\usepackage{subfig}
\usepackage{array,booktabs,multirow}
\usepackage{placeins}

\usepackage{logictools}
\usepackage{prooftheory}
\usepackage{comment}
\usepackage{mathenvironments}
\usepackage{drawproof}
\usepackage{bussproofs}
\usepackage{tensor}
\usepackage{mathtools}
\usepackage{amsmath}

\usepackage{graphicx}
%\usepackage{caption}
%\usepackage{subcaption}

\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\renewcommand{\floatpagefraction}{0.75}


\newcommand{\freevar}[1]{\mathrm{FV}(#1)}

\newcommand{\Vertices}[1]{V_{#1}}
\newcommand{\Edges}[1]{E_{#1}}
\newcommand{\Conclusion}[1]{\clause_{#1}}

\newcommand{\axiom}[1]{\widehat{#1}}
\newcommand{\n}{v}
\newcommand{\raiz}[1]{\rho(#1)}

\newcommand{\pedge}[3]{\ensuremath{\raiz{#1} \xrightarrow{#2} \raiz{#3}}}


\newcommand\inlineeqno{\stepcounter{equation}\ (\theequation)}


% Contraction
\newcommand{\con}[3]{\lfloor #1 \rfloor_{#2}^{#3}}

% Resolution
%\newcommand{\res}[6]{#1 \tensor[^{#2}_{#3}]{\odot}{^{#4}_{#5}} #6}
%\newcommand{\res}[6]{#1 \prescript{#2}{#3}{\odot^{#4}_{#5}} #6}

\newcommand{\res}[4]{\mathrel{\operatorname*{\odot}_{#1 #3}^{#2 #4}}}

\title{Partial Regularization of\\ First-Order Resolution Proofs}

\author{
  Jan Gorzny\inst{1}
  \thanks{Supported by the Google Summer of Code 2014 program.}
  \and 
  Bruno Woltzenlogel Paleo\inst{2,3}
  %\thanks{Supported by the Austrian Science Fund, project P24300.}
}

\authorrunning{J.\~Gorzny \and B.\~Woltzenlogel Paleo}

\institute{
  \email{jgorzny@uwaterloo.ca}, University of Waterloo, Canada
  \and 
  \email{bruno@logic.at}, Vienna University of Technology, Austria
  \and 
  Australian National University
}




\begin{document}

\maketitle


\begin{abstract}
This paper describes the generalization of the 
proof compression algorithm
\RecyclePivotsIntersection 
%\texttt{RecyclePivots\-WithIntersection}
from propositional to first-order logic. The generalized algorithm performs partial regularization of resolution proofs containing resolution and factoring inferences with \emph{unification}, as generated by many automated theorem provers. An empirical evaluation of the generalized algorithm and its combinations with \SFOLowerUnits is also presented.
\end{abstract}


\setcounter{footnote}{0}

\section{Introduction} 

First-order automated theorem provers, commonly based on resolution and superposition calculi, have recently achieved a high degree of maturity. Proof production is a key feature that has been gaining importance, since proofs are crucial for applications that require certification of a prover's answers or information extractable from proofs (e.g. unsat cores, interpolants, instances of quantified variables). Nevertheless, proof production is non-trivial \cite{SchulzAPPA}, and the best, most efficient provers do not necessarily generate the best, least redundant proofs.

For proofs using propositional resolution generated by SAT- and SMT-solvers, there is a wide variety of proof compression techniques. Algebraic properties of the resolution operation that might be useful for compression were investigated in \cite{bwp10}.
Compression algorithms based on rearranging and sharing chains of resolution inferences have been
developed in \cite{Amjad07} and \cite{Sinz}.  Cotton \cite{CottonSplit} proposed an algorithm that
compresses a refutation by repeatedly splitting it into a proof of a heuristically chosen literal $\ell$
and a proof of $\dual{\ell}$, and then resolving them to form a new refutation.  The {\ReduceReconstruct} algorithm \cite{RedRec} searches for locally redundant
subproofs that can be rewritten into subproofs of stronger clauses and with fewer resolution steps.
A linear time proof compression algorithm based on partial
regularization was proposed in \cite{RP08} and improved in \cite{LURPI}.

In contrast, there has been much less work on simplifying first-order proofs. For tree-like sequent calculus proofs, algorithms based on cut-introduction \cite{BrunoLPAR,Hetzl} have been proposed. However, converting a DAG-like resolution or superposition proof, as usually generated by current provers, into a tree-like sequent calculus proof may increase the size of the proof. For arbitrary proofs in the TPTP \cite{TPTP} format (including DAG-like first-order resolution proofs), there is a simple algorithm \cite{LPARCzech} that looks for terms that occur often in any TSTP \cite{TPTP} proof and introduces abbreviations for these terms. 


The work reported in this paper is part of a new trend that aims at lifting successful propositional proof compression algorithms to first-order logic. Our first target was the propositional {\LowerUnits} algorithm, which delays resolution steps with unit clauses, resulting in the
{\SFOLowerUnits} 
({\GFOLU}) algorithm \cite{GFOLU}. Here we continue this line of research by lifting the 
%\RecyclePivotsIntersection 
\texttt{Recycle\-PivotsWithIntersection}
({\RPI}) algorithm \cite{LURPI}, which is an improvement of the \texttt{RecyclePivots} ({\RP}) algorithm \cite{RP08}, providing better compression on proofs where nodes have several children. 

%TODO: edit this again
Section \ref{sec:res} introduces the first-order resolution calculus and the notations used in this paper. Section \ref{sec:Challenges} discusses the challenges that arise in the first-order case (mainly due to unification), which are not present the propositional case. Section \ref{sec:FORPI} describes an algorithm that overcomes these challenges. Section \ref{sec:exp} presents experimental results obtained by applying this algorithm, and its combinations with {\GFOLU}, on hundreds of proofs generated with the {\SPASS} theorem prover. Section \ref{sec:conclusion} concludes the paper.



\input{sec-Resolution}

\section{The Propositional Algorithm}

{\RPI} (formally defined in Appendix \ref{Section:RPI}) removes \emph{irregularities}, which are resolution inferences with a node $\eta$ when the resolved literal (a.k.a. \emph{pivot}) occurs as the pivot of another inference located below in the path from $\eta$ to the root of the proof. In the worst case, regular resolution proofs can be exponentially bigger than irregular ones, but {\RPI} takes care of regularizing the proof only partially, removing inferences only when this does not enlarge the proof.

%ToDo: Informal textual description of the propositional algorithm, explaining what safe literals are. 
{\RPI} traverses the proof twice. On the first traversal (bottom-up), it stores for each node a set of \emph{safe literals} that are resolved in all paths below it in the proof or that occur in the root clause of the proof. If one of the node's resolved literals belongs to the set of safe literals, then it is possible to \emph{regularize} the node by replacing it by the parent containing the safe literal. To do this replacement efficiently, the replacement is postponed by marking the other parent as a \texttt{deletedNode}. Then, on a single second traversal (top-down), regularization is performed: any node that has a parent node marked as a \texttt{deletedNode} is replaced by its other parent.
%Refer reader to the CADE 2011 paper (where RPI is described) for a formal description of the propositional algorithm. 
% contains a formal description of {\RPI} (taken from \cite{LURPI}).
%Consider adding the formal description to an appendix in this paper, for the convenience of the reviewer.

The {\RPI} and the {\RP} algorithms differ from each other mainly in the
computation of the safe literals of a node that has many children. While the former 
returns the intersection as shown in Algorithm~\ref{algo:SetSafeLiterals}, the latter
returns the empty set. 
Moreover, while in {\RPI} the safe literals of the root node contain all the literals of the root clause, in {\RP} the root node is always assigned an empty set of literals. 

\input{sec-Challenges}
\input{sec-FORPI}
\input{sec-Exp}


\section{Conclusions and Future Work}\label{sec:conclusion}
\vspace{-0.25cm}
The main contribution of this paper is the generalization of the propositional proof compression algorithm {\RPI} to the first-order case. As indicated in Section \ref{sec:Challenges}, the generalization is challenging, because unification changes the pivots and, consequently, must be taken into account when collecting safe literals and marking nodes for deletion.

Every computational experiment evaluates not only the algorithm but also the data on which it is executed. Although the experimental results are not as promissing as expected, this is due to the fact that the 308 proofs currently available are too short to contain a significant amount of irregularities. This is a valuable piece of information, allowing us to conclude that it is not worth applying {\FORPI} to pure resolution proofs which current state-of-the-art first-order theorem provers seem capable of producing. Nevertheless, based on our positive results for {\RPI} on much longer proofs generated by SAT and SMT solvers \cite{LURPI}, {\FORPI} remains a promising option to be revisited in the future, when the performance of first-order theorem provers catch up with advances in SAT and SMT and taller first-order benchmark proofs become available.

% These algorithms are very fast, and together they may simplify the proof considerably for a relatively quick time cost.

% {\RPI} performs best when the proofs are tall; {\FORPI} will likely perform similarly. However, the proofs in this data set are relatively short, and those compressed by {\GFOLU} first are even shorter. Thus, the performance of {\FORPI} is not surprising.

%{\FORPI} continues to support the idea of listing propositional proof compression algorithms to the first-order case. The experimental results discussed in the previous continue to be encouraging, and are consistent with trends observed in the propositional case. 

%\paragraph{Acknowledgments:}


\begin{footnotesize}
%\bibliographystyle{splncs}
\bibliographystyle{plain}
\bibliography{biblio}
\end{footnotesize}
\appendix
\input{sec-PropRPI}
\end{document}

% vim: tw=100
